{"version":3,"file":"2021_horoi_exploring-3cc2e0e0.js","sources":["../../../.svelte-kit/adapter-node/chunks/2021_horoi_exploring.js"],"sourcesContent":["const data = {\n  \"abstract\": `Recent work has established clear links between the generalization performance of trained neural networks and the geometry of their loss landscape near the local minima to which they converge. This suggests that qualitative and quantitative examination of the loss landscape geometry could yield insights about neural network generalization performance during training. To this end, researchers have proposed visualizing the loss landscape through the use of simple dimensionality reduction techniques. However, such visualization methods have been limited by their linear nature and only capture features in one or two dimensions, thus restricting sampling of the loss landscape to lines or planes. Here, we expand and improve upon these in three ways. First, we present a novel \"jump and retrain\" procedure for sampling relevant portions of the loss landscape. We show that the resulting sampled data holds more meaningful information about the network's ability to generalize. Next, we show that non-linear dimensionality reduction of the jump and retrain trajectories via PHATE, a trajectory and manifold-preserving method, allows us to visualize differences between networks that are generalizing well vs poorly. Finally, we combine PHATE trajectories with a computational homology characterization to quantify trajectory differences.`,\n  authors: [\n    \"Stefan Horoi\",\n    \"Jessie Huang\",\n    \"Bastian Rieck\",\n    \"Guillaume Lajoie\",\n    \"Guy Wolf\",\n    \"Smita Krishnaswamy\"\n  ],\n  href: \"https://arxiv.org/abs/2102.00485v2\",\n  keywords: [\n    \"3\",\n    \"5\",\n    \"Artificial neural network loss landscape\",\n    \"Guillaume Lajoie 1\",\n    \"Jessie Huang\",\n    \"Non-linear dimensionality reduction\",\n    \"Topological data analysis\"\n  ],\n  month: 1,\n  pages: \"171-184\",\n  periodical: \"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\",\n  publisher: \"Springer Science and Business Media Deutschland GmbH\",\n  selected: true,\n  title: \"Exploring the Geometry and Topology of Neural Network Loss Landscapes\",\n  type: \"Journal Article\",\n  year: 2021\n};\nexport {\n  data as default\n};\n"],"names":[],"mappings":"AAAK,MAAC,IAAI,GAAG;AACb,EAAE,UAAU,EAAE,CAAC,2zCAA2zC,CAAC;AAC30C,EAAE,OAAO,EAAE;AACX,IAAI,cAAc;AAClB,IAAI,cAAc;AAClB,IAAI,eAAe;AACnB,IAAI,kBAAkB;AACtB,IAAI,UAAU;AACd,IAAI,oBAAoB;AACxB,GAAG;AACH,EAAE,IAAI,EAAE,oCAAoC;AAC5C,EAAE,QAAQ,EAAE;AACZ,IAAI,GAAG;AACP,IAAI,GAAG;AACP,IAAI,0CAA0C;AAC9C,IAAI,oBAAoB;AACxB,IAAI,cAAc;AAClB,IAAI,qCAAqC;AACzC,IAAI,2BAA2B;AAC/B,GAAG;AACH,EAAE,KAAK,EAAE,CAAC;AACV,EAAE,KAAK,EAAE,SAAS;AAClB,EAAE,UAAU,EAAE,sIAAsI;AACpJ,EAAE,SAAS,EAAE,sDAAsD;AACnE,EAAE,QAAQ,EAAE,IAAI;AAChB,EAAE,KAAK,EAAE,uEAAuE;AAChF,EAAE,IAAI,EAAE,iBAAiB;AACzB,EAAE,IAAI,EAAE,IAAI;AACZ;;;;"}