abstract: "The ability to efficiently learn from little-to-no data is critical to\
  \ applying NLP to tasks where data collection is costly or otherwise difficult.\
  \ This is a challenging setting both academically and practically\u2014particularly\
  \ because training neutral models typically require large amount of labeled data.\
  \ More recently, advances in pretraining on unlabelled data have brought up the\
  \ potential of better zero-shot or few-shot learning (Devlin et al., 2019; Brown\
  \ et al., 2020). In particular, over the past year, a great deal of research has\
  \ been conducted to better learn from limited data using large-scale language models.\
  \ In this tutorial, we aim at bringing interested NLP researchers up to speed about\
  \ the recent and ongoing techniques for zero- and few-shot learning with pretrained\
  \ language models. Additionally, our goal is to reveal new research opportunities\
  \ to the audience, which will hopefully bring us closer to address existing challenges\
  \ in this domain."
authors:
- Iz Beltagy
- Arman Cohan
- Robert Logan IV
- Sewon Min
- Sameer Singh
project: other
title: Zero- and Few-Shot NLP with Pretrained Language Models
url: https://www.semanticscholar.org/paper/037110f8e99488f9b8f6e962da0a912d927695e5
venue: Annual Meeting of the Association for Computational Linguistics
year: 2022
