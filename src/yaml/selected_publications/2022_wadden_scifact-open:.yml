abstract: While research on scientific claim verification has led to the development
  of powerful systems that appear to approach human performance, these approaches
  have yet to be tested in a realistic setting against large corpora of scientific
  literature. Moving to this open-domain evaluation setting, however, poses unique
  challenges; in particular, it is infeasible to exhaustively annotate all evidence
  documents. In this work, we present SciFact-Open, a new test collection designed
  to evaluate the performance of scientific claim verification systems on a corpus
  of 500K research abstracts. Drawing upon pooling techniques from information retrieval,
  we collect evidence for scientific claims by pooling and annotating the top predictions
  of four state-of-the-art scientific claim verification models. We find that systems
  developed on smaller corpora struggle to generalize to SciFact-Open, exhibiting
  performance drops of at least 15 F1. In addition, analysis of the evidence in SciFact-Open
  reveals interesting phenomena likely to appear when claim verification systems are
  deployed in practice, e.g., cases where the evidence supports only a special case
  of the claim. Our dataset is available at https://github.com/dwadden/scifact-open.
authors:
- David Wadden
- Kyle Lo
- Bailey Kuehl
- Arman Cohan
- Iz Beltagy
- Lucy Lu Wang
- Hannaneh Hajishirzi
project: other
title: 'SciFact-Open: Towards open-domain scientific claim verification'
url: https://www.semanticscholar.org/paper/f13b251c8346bc3be19b71b840449831e9716999
venue: Conference on Empirical Methods in Natural Language Processing
year: 2022
