abstract: Recent studies have found that summaries generated by large language models
  (LLMs) are favored by human annotators over the original reference summaries in
  commonly used summarization datasets. Therefore, we investigate a new learning paradigm
  of text summarization models that considers the LLMs as the reference or the gold-standard
  oracle on commonly used summarization datasets such as the CNN/DailyMail dataset.
  To examine the standard practices that are aligned with the new learning setting,
  we propose a novel training method that is based on contrastive learning with LLMs
  as a summarization quality evaluator. For this reward-based training method, we
  investigate two different methods of utilizing LLMs for summary quality evaluation,
  namely GPTScore and GPTRank. Our experiments on the CNN/DailyMail dataset demonstrate
  that smaller summarization models trained by our proposed method can achieve performance
  equal to or surpass that of the reference LLMs, as evaluated by the LLMs themselves.
  This underscores the efficacy of our proposed paradigm in enhancing model performance
  over the standard maximum likelihood estimation (MLE) training method, and its efficiency
  since it only requires a small budget to access the LLMs. We release the training
  scripts, model outputs, and LLM-based evaluation results to facilitate future studies.
authors:
- Yixin Liu
- Alexander R. Fabbri
- Pengfei Liu
- Dragomir R. Radev
- Arman Cohan
project: other
title: On Learning to Summarize with Large Language Models as References
url: https://www.semanticscholar.org/paper/c8b5a244be3b6623feb7a623d9f97c183bcbdef0
venue: In ArXiv 2023
year: 2023
