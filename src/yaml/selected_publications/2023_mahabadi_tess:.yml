abstract: Diffusion models have emerged as a powerful paradigm for generation, obtaining
  strong performance in various domains with continuous-valued inputs. Despite the
  promises of fully non-autoregressive text generation, applying diffusion models
  to natural language remains challenging due to its discrete nature. In this work,
  we propose Text-to-text Self-conditioned Simplex Diffusion (TESS), a text diffusion
  model that is fully non-autoregressive, employs a new form of self-conditioning,
  and applies the diffusion process on the logit simplex space rather than the typical
  learned embedding space. Through extensive experiments on natural language understanding
  and generation tasks including summarization, text simplification, paraphrase generation,
  and question generation, we demonstrate that TESS outperforms state-of-the-art non-autoregressive
  models and is competitive with pretrained autoregressive sequence-to-sequence models.
authors:
- Rabeeh Karimi Mahabadi
- Jaesung Tae
- Hamish Ivison
- J. Henderson
- Iz Beltagy
- Matthew E. Peters
- Arman Cohan
project: other
title: 'TESS: Text-to-Text Self-Conditioned Simplex Diffusion'
url: https://www.semanticscholar.org/paper/67cdecbcfed07b9a29d9e2a92da684604383afd7
venue: In ArXiv 2023
year: 2023
