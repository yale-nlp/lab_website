abstract: There are a growing number of table pre-training methods proposed for reasoning
  over tabular data (e.g., question answering, fact checking, and faithful text generation).
  However, most existing methods are benchmarked solely on a limited number of datasets,
  varying in configuration, which leads to a lack of unified, standardized, fair,
  and comprehensive comparison between methods. This paper presents OpenRT, the first
  open-source framework for reasoning over tabular data, to reproduce existing table
  pre-training models for performance comparison and develop new models quickly. We
  implemented and compared six table pre-training models on four question answering,
  one fact checking, and one faithful text generation datasets. Moreover, to enable
  the community to easily construct new table reasoning datasets, we developed TaRAT,
  an annotation tool which supports multi-person collaborative annotations for various
  kinds of table reasoning tasks. The researchers are able to deploy the newly-constructed
  dataset to OpenRT and compare the performances of different baseline systems.
authors:
- Yilun Zhao
- Boyu Mi
- Zhenting Qi
- Linyong Nan
- Minghao Guo
- Arman Cohan
- Dragomir R. Radev
project: other
title: 'OpenRT: An Open-source Framework for Reasoning Over Tabular Data'
url: https://www.semanticscholar.org/paper/a0ee62ff9a6a00d29634a5146f9ffb91ec730e32
venue: In ACL 2023
year: 2023
